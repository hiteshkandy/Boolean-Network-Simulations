{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUND TRUTH EXPRESSION SIMULATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory shape: (50, 7)\n",
      "[[0 0 0 1 1 0 0]\n",
      " [0 1 0 1 0 1 0]\n",
      " [1 1 1 0 0 0 1]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Generic update function built from a text specification\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def update_from_text(state, network_str):\n",
    "    \"\"\"\n",
    "    state: 1D np.array of length n_vars (0/1)\n",
    "    network_str: multiline text with lines like \"v0 = v1 AND NOT v2\"\n",
    "    returns: new_state, same shape as state\n",
    "    \"\"\"\n",
    "    # parse lines\n",
    "    lines = [L.strip() for L in network_str.strip().split('\\n') if L.strip()]\n",
    "    var_names = []\n",
    "    exprs = []\n",
    "    for line in lines:\n",
    "        var, expr = [part.strip() for part in line.split('=', 1)]\n",
    "        var_names.append(var)\n",
    "        # convert operators to Python syntax\n",
    "        expr_py = (expr.replace('AND', ' and ')\n",
    "                      .replace('OR', ' or ')\n",
    "                      .replace('NOT', ' not '))\n",
    "        exprs.append(expr_py)\n",
    "    # map var name to its index in the state vector\n",
    "    var2idx = {v: i for i, v in enumerate(var_names)}\n",
    "\n",
    "    # build evaluation context\n",
    "    ctx = {var: bool(state[var2idx[var]]) for var in var_names}\n",
    "\n",
    "    # evaluate each expression\n",
    "    new = np.zeros_like(state)\n",
    "    for i, expr_py in enumerate(exprs):\n",
    "        # eval returns a Python bool, convert to int\n",
    "        new[i] = int(eval(expr_py, {}, ctx))\n",
    "    return new\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Simulator\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def simulate_from_text(network_str, initial_state, steps=50):\n",
    "    \"\"\"\n",
    "    network_str: multiline network definition\n",
    "    initial_state: list or 1D array of 0/1 of length n_vars\n",
    "    steps: number of synchronous update steps\n",
    "    returns: history, a (steps × n_vars) np.array\n",
    "    \"\"\"\n",
    "    state = np.array(initial_state, dtype=int)\n",
    "    history = np.zeros((steps, state.size), dtype=int)\n",
    "    for t in range(steps):\n",
    "        history[t] = state\n",
    "        state = update_from_text(state, network_str)\n",
    "    return history\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Example usage\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "'''Emter input network in this format:'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net_text = '''\n",
    "    v0 = v1 AND NOT v2\n",
    "    v1 = v0 OR v3\n",
    "    v2 = NOT v4\n",
    "    v3 = v2 OR NOT v5\n",
    "    v4 = v3 AND v6\n",
    "    v5 = NOT v1 OR v4\n",
    "    v6 = v5 AND NOT v0\n",
    "    '''\n",
    "    \n",
    "    # determine number of variables from network definition\n",
    "    lines = [L.strip() for L in net_text.strip().split('\\n') if L.strip()]\n",
    "    n_vars = len(lines)\n",
    "\n",
    "    # pick a random initial state of length n_vars\n",
    "    init = np.random.randint(0, 2, size=n_vars)\n",
    "    traj = simulate_from_text(net_text, init, steps=50)\n",
    "\n",
    "    print(\"Trajectory shape:\", traj.shape)\n",
    "    print(traj[:10])  # first 10 states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCORING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_canalization_depth 1.8571428571428572 \n",
      "\n",
      "average_redundancy 0.0 \n",
      "\n",
      "average_sensitivity 0.5714285714285714 \n",
      "\n",
      "number_of_ffls 0 \n",
      "\n",
      "normalized_ffls 0.0 \n",
      "\n",
      "number_of_fbls 11 \n",
      "\n",
      "normalized_fbls 0.2619047619047619 \n",
      "\n",
      "criticality 1.058 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# Adjust this to point at your copy of the toolbox\n",
    "sys.path.append('/Users/hiteshkandarpa/Desktop/Acads/AlgoBio/Project/Code/DesignPrinciplesGeneNetworks')\n",
    "from lib import canalizing_function_toolbox_v16 as cft\n",
    "\n",
    "\n",
    "def input_redundancy(f):\n",
    "    n = int(np.log2(len(f)))\n",
    "    if n == 0:\n",
    "        return 1.0\n",
    "    relevant = 0\n",
    "    for i in range(n):\n",
    "        for x in itertools.product([0,1], repeat=n):\n",
    "            idx = sum(x[n-1-j] << j for j in range(n))\n",
    "            y = list(x); y[i] ^= 1\n",
    "            jdx = sum(y[n-1-j] << j for j in range(n))\n",
    "            if f[idx] != f[jdx]:\n",
    "                relevant += 1\n",
    "                break\n",
    "    return 1 - (relevant / n)\n",
    "\n",
    "\n",
    "def average_sensitivity(f):\n",
    "    n = int(np.log2(len(f)))\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    total = 0\n",
    "    for x in itertools.product([0,1], repeat=n):\n",
    "        idx = sum(x[n-1-i] << i for i in range(n))\n",
    "        for i in range(n):\n",
    "            y = list(x); y[i] ^= 1\n",
    "            jdx = sum(y[n-1-k] << k for k in range(n))\n",
    "            total += abs(f[idx] - f[jdx])\n",
    "    return total / (n * 2**n)\n",
    "\n",
    "\n",
    "class BooleanNetworkAnalyzer:\n",
    "    def __init__(self, network_str):\n",
    "        self.nodes = {}\n",
    "        self.constants = []\n",
    "        self.truth_tables = {}\n",
    "        self.variables = []\n",
    "        self._parse_network(network_str)\n",
    "        self._build_truth_tables()\n",
    "        self._build_adjacency_matrix()\n",
    "\n",
    "    def _parse_network(self, network_str):\n",
    "        lines = [L.strip() for L in network_str.strip().split('\\n') if L.strip()]\n",
    "        for line in lines:\n",
    "            if '=' not in line: continue\n",
    "            var, expr = line.split('=', 1)\n",
    "            self.nodes[var.strip()] = expr.strip()\n",
    "\n",
    "        defined = set(self.nodes.keys())\n",
    "        used = set()\n",
    "        for expr in self.nodes.values():\n",
    "            used |= set(re.findall(r'\\b[A-Za-z_][A-Za-z0-9_]*\\b', expr))\n",
    "        # external symbols\n",
    "        self.constants = sorted(used - defined)\n",
    "        self.variables = list(self.nodes.keys())\n",
    "\n",
    "    def _evaluate_expression(self, expr, ctx):\n",
    "        e = (expr.replace('AND',' and ')\n",
    "                 .replace('OR',' or ')\n",
    "                 .replace('NOT',' not '))\n",
    "        try:\n",
    "            return int(eval(e, {}, ctx))\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    def _build_truth_tables(self):\n",
    "        for node, expr in self.nodes.items():\n",
    "            deps = sorted(set(re.findall(r'\\b[A-Za-z_][A-Za-z0-9_]*\\b', expr))\n",
    "                          - set(self.constants))\n",
    "            table = []\n",
    "            for vals in itertools.product([0,1], repeat=len(deps)):\n",
    "                ctx = dict(zip(deps, vals))\n",
    "                for c in self.constants:\n",
    "                    ctx[c] = 0\n",
    "                table.append(self._evaluate_expression(expr, ctx))\n",
    "            self.truth_tables[node] = table\n",
    "\n",
    "    def _build_adjacency_matrix(self):\n",
    "        n = len(self.variables)\n",
    "        A = np.zeros((n, n), dtype=int)\n",
    "        for i, targ in enumerate(self.variables):\n",
    "            for j, src in enumerate(self.variables):\n",
    "                if src in re.findall(r'\\b[A-Za-z_][A-Za-z0-9_]*\\b',\n",
    "                                     self.nodes[targ]):\n",
    "                    A[i, j] = 1\n",
    "        self.adjacency_matrix = A\n",
    "\n",
    "    def compute_scores(self):\n",
    "        F = [self.truth_tables[v] for v in self.variables]\n",
    "        I = [[j for j,val in enumerate(row) if val]\n",
    "             for row in self.adjacency_matrix]\n",
    "        return self._calculate_network_metrics(F, I)\n",
    "\n",
    "    def _calculate_network_metrics(self, F, I):\n",
    "        scores = {}\n",
    "        N = len(F)\n",
    "        if N < 3:\n",
    "            print(\"Warning: too few nodes for FFL/FBL.\")\n",
    "            return None\n",
    "\n",
    "        # canalization\n",
    "        depths = [cft.find_layers(f)[0] for f in F]\n",
    "        scores['average_canalization_depth'] = np.mean(depths)\n",
    "        scores['average_redundancy']       = np.mean([input_redundancy(f) for f in F])\n",
    "        scores['average_sensitivity']      = np.mean([average_sensitivity(f) for f in F])\n",
    "\n",
    "        # graph and motifs\n",
    "        A = self.adjacency_matrix\n",
    "        G = nx.from_numpy_array(A, create_using=nx.DiGraph)\n",
    "\n",
    "        # FFLs\n",
    "        try:\n",
    "            ffls, ffl_types = cft.get_ffls(A, F, I)\n",
    "            scores['number_of_ffls']     = len(ffls)\n",
    "            total_triplets = (N*(N-1)*(N-2)) / 6\n",
    "            scores['normalized_ffls']    = len(ffls) / total_triplets\n",
    "        except Exception as e:\n",
    "            print(\"FFL computation failed:\", e)\n",
    "            scores['number_of_ffls']  = 0\n",
    "            scores['normalized_ffls'] = 0\n",
    "\n",
    "        # FBLs\n",
    "        try:\n",
    "            cycles = [c for c in nx.simple_cycles(G) if len(c)>1]\n",
    "            scores['number_of_fbls']     = len(cycles)\n",
    "            total_edges = N*(N-1)\n",
    "            scores['normalized_fbls']    = len(cycles) / total_edges\n",
    "        except Exception as e:\n",
    "            print(\"FBL computation failed:\", e)\n",
    "            scores['number_of_fbls']  = 0\n",
    "            scores['normalized_fbls'] = 0\n",
    "\n",
    "        # criticality\n",
    "        try:\n",
    "            scores['criticality'] = cft.derrida_value(F, I, N, 1)\n",
    "        except Exception as e:\n",
    "            print(\"Criticality failed:\", e)\n",
    "            scores['criticality'] = 0\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''enter network here in this format:, this is some example'''\n",
    "\n",
    "    network_input = '''\n",
    "v0 = v1 AND NOT v2 \n",
    "v1 = v0 OR v3 \n",
    "v2 = NOT v4 \n",
    "v3 = v2 OR NOT v5 \n",
    "v4 = v3 AND v6 \n",
    "v5 = NOT v1 OR v4 \n",
    "v6 = v5 AND NOT v0\n",
    "'''\n",
    "    analyzer = BooleanNetworkAnalyzer(network_input)\n",
    "    scores = analyzer.compute_scores()\n",
    "    for keys,values in scores.items():\n",
    "        print(keys, values, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTANCE SCORE\n",
    "\n",
    "There are 2 scores here, Overlap and Hamming dist. (both normalised - verified by using same network for data generation and scoring-got 1.00 each score as expected)\n",
    "# Overlap \n",
    "The fraction of your “experimental” states that actually show up in the simulated trajectory, in the same order they appear in the experiment. \n",
    "How well the model is able to visit all the key states you observed, without caring exactly when it hits them, only that it does so in the right sequence.\n",
    "\n",
    "# Hamming\n",
    "One minus the average fraction of bits that differ, when you line up the first 𝑇 steps of simulation with the 𝑇 experimental steps.\n",
    "How similarly the network behaves at each time point, measuring bit-by-bit agreement.\n",
    "\n",
    "\n",
    "Overlap tells you about coverage and order.\n",
    "Hamming tells you about synchrony and bit-perfect matching.\n",
    "\n",
    "# Why they can differ dramatically\n",
    "\n",
    "1. High overlap, low Hamming:\n",
    "Your model eventually hits all the important states (overlap → 1.0), but maybe it reaches them at different times or with detours. When you compare step-by-step, many time points will be “wrong,” so the Hamming score drops.\n",
    "\n",
    "2. High Hamming, low overlap:\n",
    "The simulation might stay very close to the experiment in the early steps (high Hamming for the first few), but then drift and never visit some later experimental states—so those missing states drive the overlap score down.\n",
    "\n",
    "3. Both high:\n",
    "Ideal case where the model both follows the experiment closely at each step and visits all experimental states in the right order.\n",
    "\n",
    "4. Both low:\n",
    "The model neither tracks the experiment step-by-step nor covers all of its key states.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap score: 1.000\n",
      "Hamming score: 1.000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 1) extract each node’s input variables from the original expressions\n",
    "dependencies = {}\n",
    "for var, expr in analyzer.nodes.items():\n",
    "    deps = sorted(\n",
    "        set(re.findall(r'\\b[A-Za-z_][A-Za-z0-9_]*\\b', expr))\n",
    "        - set(analyzer.constants)\n",
    "    )\n",
    "    dependencies[var] = deps\n",
    "\n",
    "# 2) simulate the Boolean network synchronously\n",
    "def simulate_network(analyzer, initial_state, steps):\n",
    "    \"\"\"\n",
    "    Returns a list of states (each a list of 0/1) of length `steps`,\n",
    "    starting from `initial_state`.  States are ordered according to\n",
    "    analyzer.variables.\n",
    "    \"\"\"\n",
    "    variables = analyzer.variables\n",
    "    truth_tables = analyzer.truth_tables\n",
    "    sim = [initial_state.copy()]\n",
    "    for _ in range(steps - 1):\n",
    "        ctx = dict(zip(variables, sim[-1]))\n",
    "        nxt = []\n",
    "        for var in variables:\n",
    "            bits = [ctx[d] for d in dependencies[var]]\n",
    "            # build the index into the truth table\n",
    "            idx = sum(bit << (len(bits) - 1 - i) for i, bit in enumerate(bits))\n",
    "            nxt.append(truth_tables[var][idx])\n",
    "        sim.append(nxt)\n",
    "    return sim\n",
    "\n",
    "# 3) compare an “expression” STG to a simulated STG\n",
    "def compare_tgs(expression_states, simulated_states):\n",
    "    \"\"\"\n",
    "    expression_states: list of length T of states, or a (T, n) array\n",
    "    simulated_states: list of length ≥ T of states, or a (S, n) array\n",
    "    Returns (overlap_score, hamming_score).\n",
    "    \"\"\"\n",
    "    # — coerce to np.arrays —\n",
    "    expr_arr = np.array(expression_states, dtype=int)     # shape (T, n)\n",
    "    sim_arr  = np.array(simulated_states, dtype=int)      # shape (S, n)\n",
    "    T, n_vars = expr_arr.shape\n",
    "    S, _      = sim_arr.shape\n",
    "\n",
    "    # — overlap score: fraction of expr states found in order in sim_arr —\n",
    "    last_j = 0\n",
    "    matched = 0\n",
    "    for i in range(T):\n",
    "        for j in range(last_j, S):\n",
    "            if np.array_equal(sim_arr[j], expr_arr[i]):\n",
    "                matched += 1\n",
    "                last_j = j + 1\n",
    "                break\n",
    "    overlap_score = matched / T\n",
    "\n",
    "    # — hamming score: compare first T steps (or S if shorter) —\n",
    "    L = min(T, S)\n",
    "    # compute Hamming distances row-wise\n",
    "    dists = np.sum(np.abs(sim_arr[:L] - expr_arr[:L]), axis=1)\n",
    "    avg_hd = dists.mean()\n",
    "    hamming_score = 1.0 - (avg_hd / n_vars)\n",
    "\n",
    "    return overlap_score, hamming_score\n",
    "\n",
    "\n",
    "# replace these with your real data:\n",
    "traj = np.array(traj)             # if traj was some array‐like\n",
    "expression_states = traj.tolist() # or you can just keep it as array\n",
    "initial_state = init              # ensure this is a list or 1D array of length n_vars\n",
    "\n",
    "T = len(expression_states)\n",
    "sim_STG = simulate_network(analyzer, initial_state, 5 * T)\n",
    "\n",
    "ov, hd = compare_tgs(expression_states, sim_STG)\n",
    "print(f\"Overlap score: {ov:.3f}\")\n",
    "print(f\"Hamming score: {hd:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
